{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODER\n",
    "\n",
    "Các lớp attention có thể truy cập tất cả các từ trong câu ban đầu. Những mô hình này thường có đặc trưng là chú ý “hai chiều” và thường được gọi là mô hình auto-encoding hay mã hóa tự động.\n",
    "\n",
    "Việc huấn luyện trước các mô hình này thường xoay quanh việc phá vỡ một câu đã cho bằng cách nào đó (ví dụ: bằng cách che các từ ngẫu nhiên trong đó) và yêu cầu mô hình tìm hoặc tái tạo lại câu ban đầu.\n",
    "\n",
    "\n",
    "ALBERT\n",
    "\n",
    "BERT\n",
    "\n",
    "DistilBERT\n",
    "\n",
    "ELECTRA\n",
    "\n",
    "RoBERTa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DECODER\n",
    "\n",
    "Các lớp attention chỉ có thể truy cập các từ được đặt trước nó trong câu. Những mô hình này thường được gọi là mô hình auto-regressive hay hồi quy tự động.\n",
    "\n",
    "\n",
    "CTRL\n",
    "\n",
    "GPT\n",
    "\n",
    "GPT-2\n",
    "\n",
    "Transformer XL\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODER - DECODER (chuổi sang chuổi)\n",
    "\n",
    "huấn luyện trước các mô hình là sử dụng các hàm mục tiêu của mô hình mã hóa hoặc giải mã, nhưng thường liên quan đến một thứ phức tạp hơn một chút. Ví dụ: T5 được huấn luyện trước bằng cách thay thế các khoảng văn bản ngẫu nhiên (có thể chứa một số từ) bằng cách che lại bằng một từ đặc biệt và mục tiêu sau đó là dự đoán phần bị che lại bởi một từ đặc biệt đó.\n",
    "\n",
    "BART\n",
    "\n",
    "mBART\n",
    "\n",
    "Marian\n",
    "\n",
    "T5"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
