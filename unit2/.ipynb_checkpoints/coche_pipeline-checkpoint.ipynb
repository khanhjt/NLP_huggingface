{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C∆† CH·∫æ HO·∫†T ƒê·ªòNG C·ª¶A PIPELINE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "classifier = pipeline(\"sentiment-analysis\")\n",
    "classifier(\n",
    "    [\n",
    "        \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "        \"I hate this so much!\",\n",
    "    ]\n",
    ") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "cho output l√† :\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "[{'label': 'POSITIVE', 'score': 0.9598047137260437},\n",
    " {'label': 'NEGATIVE', 'score': 0.9994558095932007}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PIPELINE g·ªìm 3 b∆∞·ªõc : ti·ªÅn x·ª≠ l√Ω, ƒë∆∞a c√°c ƒë·∫ßu v√†o qua m√¥  h√¨nh v√† h·∫≠u x·ª≠ l√Ω."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/full_nlp_pipeline-dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## B∆Ø·ªöC 1: Ti·ªÅn x·ª≠ l√Ω v·ªõi m·ªôt tokenizer\n",
    "\n",
    "ƒê·∫ßu ti√™n chuy·ªÉn ƒë·∫ßu v√†o vƒÉn b·∫£n th√†nh d·∫°ng s·ªë ƒë·ªÉ m√¥ h√¨nh hi·ªÉu, ƒë·ªÉ l√†m ƒëi·ªÅu n√†y ta d√πng tokenizer:\n",
    "\n",
    "        - T√°ch ƒë·∫ßu v√†o th√†nh c√°c t·ª´, t·ª´ ph·ª•, ho·∫∑c k√Ω hi·ªáu ƒëc g·ªçi l√† tokens.\n",
    "        - √Ånh x·∫° m·ªói token th√†nh m·ªôt s·ªë nguy√™n\n",
    "        -Th√™m ƒë·∫ßu v√†o b·ªï sung c√≥ th·ªÉ h·ªØu √≠ch cho m√¥ h√¨nh.\n",
    "\n",
    "Ta c·∫ßn x·ª≠ l√Ω gi·ªëng nh∆∞ khi m√¥ h√¨nh ƒëc hu·∫•n luy·ªán tr∆∞·ªõc, c√≥ th·ªÉ s·ª≠ d·ª•ng l·ªõp AutoTokenizer v√† ph∆∞∆°ng th·ª©c from_pretrained(). S·ª≠ d·ª•ng checkpoint c·ªßa m√¥ h√¨nh , n√≥ s·∫Ω t·ª± t√¨m n·∫°p d·ªØ li·ªáu ƒë∆∞·ª£c li√™n k·∫øt v·ªõi tokenizer c·ªßa m√¥ h√¨nh v√† l∆∞u cache.(ch·ªâ t·∫£i ·ªü l·∫ßn ch·∫°y ƒë·∫ßu ti√™n).\n",
    "\n",
    "Checkpoint m·∫∑c ƒë·ªãnh c·ªßa sentiment-analysis l√† distilbert-base-unsased-finetuned-...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Khi c√≥ tokenizer, ta c√≥ th·ªÉ truy·ªÉn tr·ª±c ti·∫øp c√°c c√¢u c·ªßa m√¨nh  v√†o b√™n trong v√† nh·∫≠n l·∫°i m·ªôt t·ª´ ƒëi·ªÉn ƒë√£ s·∫µn s√†ng ƒë·ªÉ cung c·∫•p cho m√¥ h√¨nh. Chuy·ªÉn c√°c ID ƒë·∫ßu v√†o th√†nh tensor.\n",
    "\n",
    "C√°c m√¥ h√¨nh transformers ch·ªâ nh√¢n tensor l√† ƒë·∫ßu v√†o\n",
    "\n",
    "ƒê·ªÉ ch·ªâ ƒë·ªãnh lo·∫°i tensor tr·∫£ v·ªÅ , d√πng return_tensors:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_inputs = [\n",
    "    \"I've been waiting for a HuggingFace course my whole life.\",\n",
    "    \"I hate this so much!\",\n",
    "]\n",
    "inputs = tokenizer(raw_inputs, padding=True, truncation=True, return_tensors=\"pt\")\n",
    "print(inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "B·∫°n c√≥ th·ªÉ chuy·ªÉn m·ªôt c√¢ th√†nh danh s√°ch c√°c c√¢u nh·ªè, ch·ªâ ƒë·ªãnh c√°c lo·∫°i tensors m√¨nh mu·ªën n·∫øu ko n√≥ s·∫Ω tr·∫£ k·∫øt qu·∫£ l√† m·ªôt danh s√°ch\n",
    " \n",
    " \n",
    "tensor Pytorch:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "{\n",
    "    'input_ids': tensor([\n",
    "        [  101,  1045,  1005,  2310,  2042,  3403,  2005,  1037, 17662, 12172, 2607,  2026,  2878,  2166,  1012,   102],\n",
    "        [  101,  1045,  5223,  2023,  2061,  2172,   999,   102,     0,     0,     0,     0,     0,     0,     0,     0]\n",
    "    ]), \n",
    "    'attention_mask': tensor([\n",
    "        [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1],\n",
    "        [1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "    ])\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ti·∫øp theo l√† cho ƒë·∫ßu ra ƒëi qua Models\n",
    "\n",
    "T·∫£i xu·ªëng m√¥ h√¨nh ƒëc hu·∫•n luy·ªán tr∆∞·ªõc.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModel.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ki·∫øn tr√∫c n√†y ch·ª©a modun Transformers c∆° s·ªü: v·ªõi m·ªôt s·ªë ƒë·∫ßu v√†o n√≥ xu·∫•t ra hidden states(ƒë·∫∑c tr∆∞ng). M·ªói ƒë·∫ßu v√†o ta s·∫Ω truy xu·∫•t m·ªôt vector ƒëa chi·ªÅu ƒë·∫°i di·ªán cho s·ª± hi·ªÉu theo ng·ªØ c·∫£nh c·ªßa ƒë·∫ßu v√†o ƒë√≥ b·∫±ng m√¥ h√¨nh Transformers.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Vector ƒëa chi·ªÅu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·∫ßu ra vector c·ªßa modun Transformers th∆∞·ªùng l·ªõn v·ªõi 3 chi·ªÅu:\n",
    "\n",
    "    - Batch size: S·ªë chu·ªói ƒë∆∞·ª£c x·ª≠ l√Ω t·∫°i 1 th·ªùi ƒëi·ªÉm.\n",
    "    - ƒê·ªô d√†i chu·ªói(shape): ƒê·ªô d√¨a bi·ªÉu di·ªÖn s·ªë vector c·ªßa chu·ªói.\n",
    "    - K√≠ch th∆∞·ªõc ·∫•n: K√≠ch th∆∞·ªõc vector c·ªßa m·ªói ƒë·∫ßu v√†o m√¥ h√¨nh.\n",
    "\n",
    "K√≠ch th∆∞·ªõc ·∫©n c√≥ th·ªÉ r·∫•t l·ªõn: 768 ho·∫∑c c√°c m√¥ h√¨nh l·ªõn c√≥ th·ªÉ ƒë·∫°t t·ªõi 3072)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ƒê·∫ßu m√¥ h√¨nh\n",
    "\n",
    "L·∫•y vector ƒëa chi·ªÅu c·ªßa c√°c tr·∫°ng th√°i ·∫©n l√†m ƒë·∫ßu v√†o v√† chi·∫øu ch√∫ng l√™n 1 chi·ªÅu kh√°c. N√≥ th∆∞·ªùng bao g·ªìm 1 ho·∫∑c nhi·ªÅu l·ªõp tuy·ªÉn t√≠nh.\n",
    "\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter2/transformer_and_head-dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·∫ßu ra c·ªßa m√¥ h√¨nh Transformer ƒë∆∞·ª£c g·ª≠i tr·ª±c ti·∫øp ƒë·∫øn ƒë·∫ßu m√¥ h√¨nh ƒë·ªÉ ƒë∆∞·ª£c x·ª≠ l√Ω.\n",
    "\n",
    "·ªû tr√™n, m√¥ h√¨nh ƒë∆∞·ª£c bi·ªÉu di·ªÖn b·∫±ng l·ªõp nh√∫ng c·ªßa n√≥ v√† c√°c Layer ti·∫øp theo. L·ªõp Embedding chuy·ªÉn ƒë·ªïi m·ªói ID trong ƒë·∫ßu v√†o ƒë∆∞·ª£c m√£ h√≥a th√†nh m·ªôt vector ƒë·∫°i di·ªán cho token ƒë∆∞·ª£c li√™n k·∫øt. C√°c l·ªõp ti·∫øp theo thao t√°c v·ªõi vector b·∫±ng c√°ch s·ª≠ d·ª•ng c∆° ch·∫ø Atten ƒë·ªÉ t·∫°o ra bi·ªÉu di·ªÖn cu·ªëi c√πng c·ªßa c√°c c√¢u.\n",
    "\n",
    "C√°c ki·∫øn tr√∫c trong Transformers:\n",
    "\n",
    "    - *Model (truy xu·∫•t c√°c tr·∫°ng th√°i ·∫©n)\n",
    "    - *ForCausalLM\n",
    "    - *ForMaskedLM\n",
    "    - *ForMultipleChoice\n",
    "    - *ForQuestionAnswering\n",
    "    - *ForSequenceClassification\n",
    "    - *ForTokenClassification\n",
    "and others ü§ó"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V·ªõi v√≠ d·ª• c·ªßa m√¨nh, ch√∫ng ta s·∫Ω c·∫ßn m·ªôt m√¥ h√¨nh c√≥ ƒë·∫ßu ph√¢n lo·∫°i tu·∫ßn t·ª± (ƒë·ªÉ c√≥ th·ªÉ ph√¢n lo·∫°i c√°c c√¢u l√† kh·∫≥ng ƒë·ªãnh ho·∫∑c ph·ªß ƒë·ªãnh). V√¨ v·∫≠y, ta s·∫Ω kh√¥ng s·ª≠ d·ª•ng l·ªõp AutoModel m√† l√† AutoModelForSequenceClassification:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSequenceClassification\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "model = AutoModelForSequenceClassification.from_pretrained(checkpoint)\n",
    "outputs = model(**inputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ƒê·∫ßu m√¥ h√¨nh l·∫•y c√°c vector ƒëa chi·ªÅu m√† ch√∫ng ta th·∫•y tr∆∞·ªõc ƒë√¢y v√† xu·∫•t ra c√°c vector c√≥ ch·ª©a 2 gi√° tr·ªã(m·ªói gi√° tr·ªã l√† 1 nh√£n)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## H·∫≠u x·ª≠ l√Ω"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(outputs.logits.shape)\n",
    "\n",
    "#ket qua\n",
    " \n",
    " \n",
    "torch.Size([2, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "V√¨ ch√∫ng ta ch·ªâ c√≥ hai c√¢u v√† hai nh√£n, k·∫øt qu·∫£ nh·∫≠n ƒë∆∞·ª£c t·ª´ m√¥ h√¨nh c·ªßa ch√∫ng ta l√† d·∫°ng 2 x 2.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
