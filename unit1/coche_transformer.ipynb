{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LỊCH SỬ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![lịch sử](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_chrono-dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transformers là mô hình ngôn ngữ\n",
    "\n",
    "Chúng đã được huấn luyện trên một lượng lớn văn bản thô theo phương pháp tự giám sát. Học tự giám sát là một loại hình huấn luyện trong đó mục tiêu được tính toán tự động từ các đầu vào của mô hình. Điều đó có nghĩa là con người không cần thiết phải gắn nhãn dữ liệu\n",
    "\n",
    "Loại mô hình này phát triển theo sự hiểu biết thống kê về ngôn ngữ mà nó đã được huấn luyện, nhưng nó không hữu ích lắm cho các tác vụ cụ thể trong thực tế. Do đó, mô hình được huấn luyện chung chung trước sau đó sẽ trải qua một quá trình được gọi là học chuyển giao(Transfer Learning). Trong quá trình này, mô hình được tinh chỉnh theo cách có giám sát - nghĩa là sử dụng các nhãn do con người gán - trên một tác vụ nhất định.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ về một tác vụ có thể kể đến dự đoán từ tiếp theo trong một câu đã đọc cho trước n từ trước đó. Đây được gọi là mô hình ngôn ngữ nhân quả bởi vì đầu ra phụ thuộc vào các đầu vào trong quá khứ và hiện tại, nhưng không phụ thuộc vào các đầu vào trong tương lai.\n",
    "![no](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/causal_modeling-dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HỌC CHUYỂN GIAO(Transfer Learning)\n",
    "\n",
    "Huấn luyện trước là hành động huấn luyện một mô hình từ đầu: các trọng số được khởi tạo ngẫu nhiên và quá trình huấn luyện bắt đầu mà không cần biết trước bất kỳ điều gì.\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/pretraining-dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ví dụ: có thể tận dụng mô hình huấn luyện trước được huấn luyện trên ngôn ngữ tiếng Anh và sau đó tinh chỉnh nó trên kho ngữ liệu arXiv, trả về một mô hình dựa trên khoa học. tinh chỉnh yêu cầu một lượng dữ liệu hạn chế kiến thức mà mô hình được huấn luyện trước được “chuyển giao”, do đó ta có thuật ngữ học chuyển giao\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/finetuning-dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ENCODER : nhận đầu vào và xây dựng biểu diễn đặc trưng.\n",
    "\n",
    "DECODER : dùng biểu diễn đặc trưng của đầu vào để tạo chuổi đích.\n",
    "\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers_blocks-dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chỉ đùng encoder: phân loại câu, nhận dạng thực thể được đặt tên\n",
    "\n",
    "Chỉ dùng decoder: tạo văn bản \n",
    "\n",
    "Encoder - Decoder: dịch, tóm tắt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "LỚP ATTENTION\n",
    "\n",
    "yêu cầu mô hình chú ý cụ thể đến các từ nhất định trong câu bạn đã chuyển khi xử lý biểu diễn của từng từ.\n",
    "\n",
    "\n",
    "một từ tự nó đã có nghĩa, nhưng nghĩa đó bị ảnh hưởng sâu sắc bởi ngữ cảnh, có thể là bất kỳ từ nào khác (hoặc các từ) trước hoặc sau từ được học."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KIẾN TRÚC GỐC\n",
    "\n",
    "![](https://huggingface.co/datasets/huggingface-course/documentation-images/resolve/main/en/chapter1/transformers-dark.svg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lớp Attention đầu tiên trong bộ giải mã chú ý đến tất cả đầu vào (quá khứ) của bộ giải mã, nhưng lớp Attention thứ hai sử dụng đầu ra của bộ mã hóa nên có thể truy cập toàn bộ câu đầu vào để dự đoán tốt nhất từ hiện tại.\n",
    "\n",
    "Attention mask cũng có thể được sử dụng trong bộ mã hóa/ giải mã để ngăn mô hình chú ý đến một số từ đặc biệt - ví dụ: từ đệm đặc biệt được sử dụng để làm cho tất cả các đầu vào có cùng độ dài khi ghép các câu lại với nhau"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KIẾN TRÚC: Đây là khung của mô hình — định nghĩa từng lớp và từng hoạt động xảy ra trong mô hình.\n",
    "\n",
    "CHECKPOINTS: Đây là những trọng số sẽ được sử dụng trong một kiến trúc nhất định.\n",
    "\n",
    "Ví dụ, BERT là 1 kiến trúc trong khi bert-base-cased,tập hợp các trọng số được huấn luyện bởi đội ngũ Google cho phiên bản đầu tiên của BERT, là 1 chekcpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
